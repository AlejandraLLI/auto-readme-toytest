{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import logging.config\n",
    "import base64\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from github import Github\n",
    "\n",
    "import src.github_utils as gu\n",
    "\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "OPEN_API_TOKEN = os.getenv(\"OPENAI_API_TOKEN\")\n",
    "PINECONE_API_TOKEN = os.getenv(\"PINECONE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<github.MainClass.Github at 0x1072a4f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Github(GITHUB_API_TOKEN)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"hyeniii/auto-readme\"\n",
    "repo = g.get_repo(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collabs = repo.get_collaborators()\n",
    "#for i in collabs:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContentFile(path=\".gitignore\"),\n",
       " ContentFile(path=\"LICENSE\"),\n",
       " ContentFile(path=\"README.md\"),\n",
       " ContentFile(path=\"config\"),\n",
       " ContentFile(path=\"requirements.txt\"),\n",
       " ContentFile(path=\"sandbox.py\"),\n",
       " ContentFile(path=\"src\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = repo.get_contents(\"\")\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.gitignore': 'LmVudgoudmVudi8KLnZzY29kZS8KKi5sb2cKKi5weWM=\\n',\n",
       " 'LICENSE': 'TUlUIExpY2Vuc2UKCkNvcHlyaWdodCAoYykgMjAyMyBIeWUgV29uIChOaWNv\\nbGUpIEh3YW5nCgpQZXJtaXNzaW9uIGlzIGhlcmVieSBncmFudGVkLCBmcmVl\\nIG9mIGNoYXJnZSwgdG8gYW55IHBlcnNvbiBvYnRhaW5pbmcgYSBjb3B5Cm9m\\nIHRoaXMgc29mdHdhcmUgYW5kIGFzc29jaWF0ZWQgZG9jdW1lbnRhdGlvbiBm\\naWxlcyAodGhlICJTb2Z0d2FyZSIpLCB0byBkZWFsCmluIHRoZSBTb2Z0d2Fy\\nZSB3aXRob3V0IHJlc3RyaWN0aW9uLCBpbmNsdWRpbmcgd2l0aG91dCBsaW1p\\ndGF0aW9uIHRoZSByaWdodHMKdG8gdXNlLCBjb3B5LCBtb2RpZnksIG1lcmdl\\nLCBwdWJsaXNoLCBkaXN0cmlidXRlLCBzdWJsaWNlbnNlLCBhbmQvb3Igc2Vs\\nbApjb3BpZXMgb2YgdGhlIFNvZnR3YXJlLCBhbmQgdG8gcGVybWl0IHBlcnNv\\nbnMgdG8gd2hvbSB0aGUgU29mdHdhcmUgaXMKZnVybmlzaGVkIHRvIGRvIHNv\\nLCBzdWJqZWN0IHRvIHRoZSBmb2xsb3dpbmcgY29uZGl0aW9uczoKClRoZSBh\\nYm92ZSBjb3B5cmlnaHQgbm90aWNlIGFuZCB0aGlzIHBlcm1pc3Npb24gbm90\\naWNlIHNoYWxsIGJlIGluY2x1ZGVkIGluIGFsbApjb3BpZXMgb3Igc3Vic3Rh\\nbnRpYWwgcG9ydGlvbnMgb2YgdGhlIFNvZnR3YXJlLgoKVEhFIFNPRlRXQVJF\\nIElTIFBST1ZJREVEICJBUyBJUyIsIFdJVEhPVVQgV0FSUkFOVFkgT0YgQU5Z\\nIEtJTkQsIEVYUFJFU1MgT1IKSU1QTElFRCwgSU5DTFVESU5HIEJVVCBOT1Qg\\nTElNSVRFRCBUTyBUSEUgV0FSUkFOVElFUyBPRiBNRVJDSEFOVEFCSUxJVFks\\nCkZJVE5FU1MgRk9SIEEgUEFSVElDVUxBUiBQVVJQT1NFIEFORCBOT05JTkZS\\nSU5HRU1FTlQuIElOIE5PIEVWRU5UIFNIQUxMIFRIRQpBVVRIT1JTIE9SIENP\\nUFlSSUdIVCBIT0xERVJTIEJFIExJQUJMRSBGT1IgQU5ZIENMQUlNLCBEQU1B\\nR0VTIE9SIE9USEVSCkxJQUJJTElUWSwgV0hFVEhFUiBJTiBBTiBBQ1RJT04g\\nT0YgQ09OVFJBQ1QsIFRPUlQgT1IgT1RIRVJXSVNFLCBBUklTSU5HIEZST00s\\nCk9VVCBPRiBPUiBJTiBDT05ORUNUSU9OIFdJVEggVEhFIFNPRlRXQVJFIE9S\\nIFRIRSBVU0UgT1IgT1RIRVIgREVBTElOR1MgSU4gVEhFClNPRlRXQVJFLgo=\\n',\n",
       " 'README.md': 'IyBBdXRvLVJlYWRtZQoKQXV0b21hdGljYWxseSBnZW5lcmF0ZSBgcmVhZG1l\\nLm1kYCBmaWxlcyBmb3IgeW91ciByZXBvc2l0b3JpZXMgYnkgcGFyc2luZyB0\\naGUgY29kZSBhbmQgc3VtbWFyaXppbmcgaXQgdXNpbmcgT3BlbmFpIEFQSS4K\\nCiMjIEZlYXR1cmVzCgotIFBhcnNlIEdpdEh1YiByZXBvc2l0b3JpZXMgd2l0\\naG91dCBkb3dubG9hZGluZyB0aGVtLgotIFN1bW1hcml6ZSBjb2RlIHVzaW5n\\nIExhbmdjaGFpbi4KLSBHZW5lcmF0ZSBodW1hbi1yZWFkYWJsZSBzdW1tYXJp\\nZXMgd2l0aCBPcGVuQUkncyBHUFQgbW9kZWwuCgojIyBQcmVyZXF1aXNpdGVz\\nCgotIFB5dGhvbiAzLjgrCi0gR2l0SHViIEFQSSBUb2tlbiAoaHR0cHM6Ly9n\\naXRodWIuY29tL3NldHRpbmdzL3Rva2VucykKLSBPcGVuQUkgQVBJIEtleSAo\\naHR0cHM6Ly9iZXRhLm9wZW5haS5jb20vc2lnbnVwKQoKIyMgSW5zdGFsbGF0\\naW9uCgoxLiBDbG9uZSBSZXBvc2l0b3J5CjIuIENyZWF0ZSBhIHZpcnR1YWwg\\nZW52aXJvbm1lbnQgYHB5dGhvbiAtbSB2ZW52IC52ZW52YCBhbmQgaW5zdGFs\\nbCBwYWNrYWdlcyBpbiBgcmVxdWlyZW1lbnRzLnR4dGAKMi4gQ3JlYXRlIC5l\\nbnYgZmlsZSBhdCB0aGUgcm9vdCBvZiBkaXJlY3RvcnkKCmBgYGJhc2gKR0lU\\nSFVCX0FQSV9UT0tFTj08YWRkLXRva2VuLWhlcmU+Ck9QRU5BSV9BUElfVE9L\\nRU49PGFkZC10b2tlbi1oZXJlPgpgYGAK\\n',\n",
       " 'config': {'config/logs': {'config/logs/local.conf': 'W2xvZ2dlcnNdCmtleXM9cm9vdCxzcmMKCltoYW5kbGVyc10Ka2V5cz1jb25z\\nb2xlSGFuZGxlcixmaWxlSGFuZGxlcgoKW2Zvcm1hdHRlcnNdCmtleXM9c2Ft\\ncGxlRm9ybWF0dGVyCgpbbG9nZ2VyX3Jvb3RdCmxldmVsPUlORk8KaGFuZGxl\\ncnM9Y29uc29sZUhhbmRsZXIsZmlsZUhhbmRsZXIKcHJvcGFnYXRlPTAKClts\\nb2dnZXJfc3JjXQpsZXZlbD1JTkZPCmhhbmRsZXJzPWNvbnNvbGVIYW5kbGVy\\nLGZpbGVIYW5kbGVyCnF1YWxuYW1lPXNyYwpwcm9wYWdhdGU9MAoKW2hhbmRs\\nZXJfY29uc29sZUhhbmRsZXJdCmNsYXNzPVN0cmVhbUhhbmRsZXIKbGV2ZWw9\\nSU5GTwpmb3JtYXR0ZXI9c2FtcGxlRm9ybWF0dGVyCmFyZ3M9KHN5cy5zdGRv\\ndXQsKQoKW2hhbmRsZXJfZmlsZUhhbmRsZXJdCmNsYXNzPUZpbGVIYW5kbGVy\\nCmxldmVsPUlORk8KZm9ybWF0dGVyPXNhbXBsZUZvcm1hdHRlcgphcmdzPSgn\\nY29uZmlnL2xvZ3MvbG9ncy5sb2cnLCkKCltmb3JtYXR0ZXJfc2FtcGxlRm9y\\nbWF0dGVyXQpmb3JtYXQ9JShhc2N0aW1lKXMgLSAlKG5hbWUpcyAtICUobGV2\\nZWxuYW1lKXMgLSAlKG1lc3NhZ2UpcwpkYXRlZm10PSVZLSVtLSVkICVIOiVN\\nOiVT\\n'}},\n",
       " 'requirements.txt': 'YWlvaHR0cD09My44LjYKYWlvc2lnbmFsPT0xLjMuMQphc3luYy10aW1lb3V0\\nPT00LjAuMwphdHRycz09MjMuMS4wCmNlcnRpZmk9PTIwMjMuNy4yMgpjZmZp\\nPT0xLjE2LjAKY2hhcnNldC1ub3JtYWxpemVyPT0zLjMuMgpjcnlwdG9ncmFw\\naHk9PTQxLjAuNQpEZXByZWNhdGVkPT0xLjIuMTQKZnJvemVubGlzdD09MS40\\nLjAKaWRuYT09My40Cm11bHRpZGljdD09Ni4wLjQKb3BlbmFpPT0wLjI4LjEK\\ncHljcGFyc2VyPT0yLjIxClB5R2l0aHViPT0yLjEuMQpQeUpXVD09Mi44LjAK\\nUHlOYUNsPT0xLjUuMApweXRob24tZGF0ZXV0aWw9PTIuOC4yCnB5dGhvbi1k\\nb3RlbnY9PTEuMC4wCnJlcXVlc3RzPT0yLjMxLjAKc2l4PT0xLjE2LjAKdHFk\\nbT09NC42Ni4xCnR5cGluZ19leHRlbnNpb25zPT00LjguMAp1cmxsaWIzPT0y\\nLjAuNwp3cmFwdD09MS4xNS4wCnlhcmw9PTEuOS4yCg==\\n',\n",
       " 'sandbox.py': 'aW1wb3J0IG9zCmltcG9ydCBhcmdwYXJzZQppbXBvcnQgbG9nZ2luZwppbXBv\\ncnQgbG9nZ2luZy5jb25maWcKCmZyb20gZG90ZW52IGltcG9ydCBsb2FkX2Rv\\ndGVudgpmcm9tIGdpdGh1YiBpbXBvcnQgR2l0aHViCmltcG9ydCBzcmMuZ2l0\\naHViX3V0aWxzIGFzIGd1Cgpsb2dnaW5nLmNvbmZpZy5maWxlQ29uZmlnKCJj\\nb25maWcvbG9ncy9sb2NhbC5jb25mIikKbG9nZ2VyID0gbG9nZ2luZy5nZXRM\\nb2dnZXIoX19uYW1lX18pCgppZiBfX25hbWVfXyA9PSAiX19tYWluX18iOgog\\nICAgcGFyc2VyID0gYXJncGFyc2UuQXJndW1lbnRQYXJzZXIoCiAgICAgICAg\\nZGVzY3JpcHRpb249IkF1dG8tcmVhZG1lIgogICAgKQogICAgcGFyc2VyLmFk\\nZF9hcmd1bWVudCgKICAgICAgICAiLXIiLCAiLS1yZXBvIiwgZGVmYXVsdD0i\\naHllbmlpaS9hdXRvLXJlYWRtZSIsIGhlbHA9IlNwZWNpZnkgZ2l0aHViIHJl\\ncG9zaXRvcnkgdG8gY3JlYXRlIGEgcmVhZG1lIG9uIgogICAgKQogICAgYXJn\\ncyA9IHBhcnNlci5wYXJzZV9hcmdzKCkKCiAgICAjIExvYWQgZW52aXJvbm1l\\nbnQgdmFyaWFibGVzIGZyb20gLmVudiBmaWxlCiAgICBsb2FkX2RvdGVudigp\\nCgogICAgR0lUSFVCX0FQSV9UT0tFTiA9IG9zLmdldGVudigiR0lUSFVCX0FQ\\nSV9UT0tFTiIpCiAgICBPUEVOX0FQSV9UT0tFTiA9IG9zLmdldGVudigiT1BF\\nTkFJX0FQSV9UT0tFTiIpCgogICAgaWYgR0lUSFVCX0FQSV9UT0tFTiBpcyBO\\nb25lIG9yIE9QRU5fQVBJX1RPS0VOIGlzIE5vbmU6CiAgICAgICAgbG9nZ2Vy\\nLmVycm9yKCJBUEkgdG9rZW5zIG5vdCBmb3VuZCBpbiBlbnZpcm9ubWVudCB2\\nYXJpYWJsZXMuIikKICAgICAgICBleGl0KDEpICAjIEV4aXQgdGhlIHByb2dy\\nYW0gaWYgdG9rZW5zIGFyZSBub3Qgc2V0CgogICAgdHJ5OgogICAgICAgIGcg\\nPSBHaXRodWIoR0lUSFVCX0FQSV9UT0tFTikKICAgICAgICByZXBvID0gZy5n\\nZXRfcmVwbygicHVibGljLWFwaXMvcHVibGljLWFwaXMiKQogICAgICAgIHBy\\naW50KHJlcG8uZ2V0X3RvcGljcygpKQogICAgICAgIGxvZ2dpbmcuaW5mbygi\\nUmVjZXRyaXZpbmcgZGF0YSBmcm9tICVzIiwgcmVwby5mdWxsX25hbWUpCiAg\\nICAgICAgY29udGVudHMgPSByZXBvLmdldF9jb250ZW50cygiIikKICAgICAg\\nICBhbGxfZmlsZXMgPSBndS5nZXRfYWxsX2ZpbGVzKHJlcG8pCiAgICAgICAg\\ncHJpbnQoYWxsX2ZpbGVzKQogICAgZXhjZXB0IEV4Y2VwdGlvbiBhcyBlOgog\\nICAgICAgIGxvZ2dpbmcuZXJyb3IoIkVycm9yIG9jY3VyZWQgaW4gcmV0cmll\\ndmluZyByZXBvIGNvbnRlbnQiLCBlKQo=\\n',\n",
       " 'src': {'src/__init__.py': '',\n",
       "  'src/github_utils.py': 'ZnJvbSBnaXRodWIgaW1wb3J0IEdpdGh1YgoKZGVmIGdldF9hbGxfZmlsZXMo\\ncmVwbywgcGF0aD0iIik6CiAgICBmaWxlcyA9IHt9CiAgICBjb250ZW50cyA9\\nIHJlcG8uZ2V0X2NvbnRlbnRzKHBhdGgpCiAgICBmb3IgY29udGVudCBpbiBj\\nb250ZW50czoKICAgICAgICBpZiBjb250ZW50LnR5cGUgPT0gImRpciI6CiAg\\nICAgICAgICAgIGZpbGVzW2NvbnRlbnQucGF0aF0gPSBnZXRfYWxsX2ZpbGVz\\nKHJlcG8sIGNvbnRlbnQucGF0aCkKICAgICAgICBlbHNlOgogICAgICAgICAg\\nICBmaWxlc1tjb250ZW50LnBhdGhdID0gY29udGVudC5jb250ZW50ICMgYmFz\\nZTY0CiAgICByZXR1cm4gZmlsZXM=\\n'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = gu.get_all_files(repo)\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.gitignore': '.env\\n.venv/\\n.vscode/\\n*.log\\n*.pyc',\n",
       " 'LICENSE': 'MIT License\\n\\nCopyright (c) 2023 Hye Won (Nicole) Hwang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n',\n",
       " 'README.md': \"# Auto-Readme\\n\\nAutomatically generate `readme.md` files for your repositories by parsing the code and summarizing it using Openai API.\\n\\n## Features\\n\\n- Parse GitHub repositories without downloading them.\\n- Summarize code using Langchain.\\n- Generate human-readable summaries with OpenAI's GPT model.\\n\\n## Prerequisites\\n\\n- Python 3.8+\\n- GitHub API Token (https://github.com/settings/tokens)\\n- OpenAI API Key (https://beta.openai.com/signup)\\n\\n## Installation\\n\\n1. Clone Repository\\n2. Create a virtual environment `python -m venv .venv` and install packages in `requirements.txt`\\n2. Create .env file at the root of directory\\n\\n```bash\\nGITHUB_API_TOKEN=<add-token-here>\\nOPENAI_API_TOKEN=<add-token-here>\\n```\\n\",\n",
       " 'config': {'config/logs': {'config/logs/local.conf': \"[loggers]\\nkeys=root,src\\n\\n[handlers]\\nkeys=consoleHandler,fileHandler\\n\\n[formatters]\\nkeys=sampleFormatter\\n\\n[logger_root]\\nlevel=INFO\\nhandlers=consoleHandler,fileHandler\\npropagate=0\\n\\n[logger_src]\\nlevel=INFO\\nhandlers=consoleHandler,fileHandler\\nqualname=src\\npropagate=0\\n\\n[handler_consoleHandler]\\nclass=StreamHandler\\nlevel=INFO\\nformatter=sampleFormatter\\nargs=(sys.stdout,)\\n\\n[handler_fileHandler]\\nclass=FileHandler\\nlevel=INFO\\nformatter=sampleFormatter\\nargs=('config/logs/logs.log',)\\n\\n[formatter_sampleFormatter]\\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\\ndatefmt=%Y-%m-%d %H:%M:%S\"}},\n",
       " 'requirements.txt': 'aiohttp==3.8.6\\naiosignal==1.3.1\\nasync-timeout==4.0.3\\nattrs==23.1.0\\ncertifi==2023.7.22\\ncffi==1.16.0\\ncharset-normalizer==3.3.2\\ncryptography==41.0.5\\nDeprecated==1.2.14\\nfrozenlist==1.4.0\\nidna==3.4\\nmultidict==6.0.4\\nopenai==0.28.1\\npycparser==2.21\\nPyGithub==2.1.1\\nPyJWT==2.8.0\\nPyNaCl==1.5.0\\npython-dateutil==2.8.2\\npython-dotenv==1.0.0\\nrequests==2.31.0\\nsix==1.16.0\\ntqdm==4.66.1\\ntyping_extensions==4.8.0\\nurllib3==2.0.7\\nwrapt==1.15.0\\nyarl==1.9.2\\n',\n",
       " 'sandbox.py': 'import os\\nimport argparse\\nimport logging\\nimport logging.config\\n\\nfrom dotenv import load_dotenv\\nfrom github import Github\\nimport src.github_utils as gu\\n\\nlogging.config.fileConfig(\"config/logs/local.conf\")\\nlogger = logging.getLogger(__name__)\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser(\\n        description=\"Auto-readme\"\\n    )\\n    parser.add_argument(\\n        \"-r\", \"--repo\", default=\"hyeniii/auto-readme\", help=\"Specify github repository to create a readme on\"\\n    )\\n    args = parser.parse_args()\\n\\n    # Load environment variables from .env file\\n    load_dotenv()\\n\\n    GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\\n    OPEN_API_TOKEN = os.getenv(\"OPENAI_API_TOKEN\")\\n\\n    if GITHUB_API_TOKEN is None or OPEN_API_TOKEN is None:\\n        logger.error(\"API tokens not found in environment variables.\")\\n        exit(1)  # Exit the program if tokens are not set\\n\\n    try:\\n        g = Github(GITHUB_API_TOKEN)\\n        repo = g.get_repo(\"public-apis/public-apis\")\\n        print(repo.get_topics())\\n        logging.info(\"Recetriving data from %s\", repo.full_name)\\n        contents = repo.get_contents(\"\")\\n        all_files = gu.get_all_files(repo)\\n        print(all_files)\\n    except Exception as e:\\n        logging.error(\"Error occured in retrieving repo content\", e)\\n',\n",
       " 'src': {'src/__init__.py': '',\n",
       "  'src/github_utils.py': 'from github import Github\\n\\ndef get_all_files(repo, path=\"\"):\\n    files = {}\\n    contents = repo.get_contents(path)\\n    for content in contents:\\n        if content.type == \"dir\":\\n            files[content.path] = get_all_files(repo, content.path)\\n        else:\\n            files[content.path] = content.content # base64\\n    return files'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_files = gu.decode_files(all_files)\n",
    "decoded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.gitignore': '.env\\n.venv/\\n.vscode/\\n*.log\\n*.pyc',\n",
       " 'LICENSE': 'MIT License\\n\\nCopyright (c) 2023 Hye Won (Nicole) Hwang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n',\n",
       " 'README.md': \"# Auto-Readme\\n\\nAutomatically generate `readme.md` files for your repositories by parsing the code and summarizing it using Openai API.\\n\\n## Features\\n\\n- Parse GitHub repositories without downloading them.\\n- Summarize code using Langchain.\\n- Generate human-readable summaries with OpenAI's GPT model.\\n\\n## Prerequisites\\n\\n- Python 3.8+\\n- GitHub API Token (https://github.com/settings/tokens)\\n- OpenAI API Key (https://beta.openai.com/signup)\\n\\n## Installation\\n\\n1. Clone Repository\\n2. Create a virtual environment `python -m venv .venv` and install packages in `requirements.txt`\\n2. Create .env file at the root of directory\\n\\n```bash\\nGITHUB_API_TOKEN=<add-token-here>\\nOPENAI_API_TOKEN=<add-token-here>\\n```\\n\",\n",
       " 'config/logs/local.conf': \"[loggers]\\nkeys=root,src\\n\\n[handlers]\\nkeys=consoleHandler,fileHandler\\n\\n[formatters]\\nkeys=sampleFormatter\\n\\n[logger_root]\\nlevel=INFO\\nhandlers=consoleHandler,fileHandler\\npropagate=0\\n\\n[logger_src]\\nlevel=INFO\\nhandlers=consoleHandler,fileHandler\\nqualname=src\\npropagate=0\\n\\n[handler_consoleHandler]\\nclass=StreamHandler\\nlevel=INFO\\nformatter=sampleFormatter\\nargs=(sys.stdout,)\\n\\n[handler_fileHandler]\\nclass=FileHandler\\nlevel=INFO\\nformatter=sampleFormatter\\nargs=('config/logs/logs.log',)\\n\\n[formatter_sampleFormatter]\\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\\ndatefmt=%Y-%m-%d %H:%M:%S\",\n",
       " 'requirements.txt': 'aiohttp==3.8.6\\naiosignal==1.3.1\\nasync-timeout==4.0.3\\nattrs==23.1.0\\ncertifi==2023.7.22\\ncffi==1.16.0\\ncharset-normalizer==3.3.2\\ncryptography==41.0.5\\nDeprecated==1.2.14\\nfrozenlist==1.4.0\\nidna==3.4\\nmultidict==6.0.4\\nopenai==0.28.1\\npycparser==2.21\\nPyGithub==2.1.1\\nPyJWT==2.8.0\\nPyNaCl==1.5.0\\npython-dateutil==2.8.2\\npython-dotenv==1.0.0\\nrequests==2.31.0\\nsix==1.16.0\\ntqdm==4.66.1\\ntyping_extensions==4.8.0\\nurllib3==2.0.7\\nwrapt==1.15.0\\nyarl==1.9.2\\n',\n",
       " 'sandbox.py': 'import os\\nimport argparse\\nimport logging\\nimport logging.config\\n\\nfrom dotenv import load_dotenv\\nfrom github import Github\\nimport src.github_utils as gu\\n\\nlogging.config.fileConfig(\"config/logs/local.conf\")\\nlogger = logging.getLogger(__name__)\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser(\\n        description=\"Auto-readme\"\\n    )\\n    parser.add_argument(\\n        \"-r\", \"--repo\", default=\"hyeniii/auto-readme\", help=\"Specify github repository to create a readme on\"\\n    )\\n    args = parser.parse_args()\\n\\n    # Load environment variables from .env file\\n    load_dotenv()\\n\\n    GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\\n    OPEN_API_TOKEN = os.getenv(\"OPENAI_API_TOKEN\")\\n\\n    if GITHUB_API_TOKEN is None or OPEN_API_TOKEN is None:\\n        logger.error(\"API tokens not found in environment variables.\")\\n        exit(1)  # Exit the program if tokens are not set\\n\\n    try:\\n        g = Github(GITHUB_API_TOKEN)\\n        repo = g.get_repo(\"public-apis/public-apis\")\\n        print(repo.get_topics())\\n        logging.info(\"Recetriving data from %s\", repo.full_name)\\n        contents = repo.get_contents(\"\")\\n        all_files = gu.get_all_files(repo)\\n        print(all_files)\\n    except Exception as e:\\n        logging.error(\"Error occured in retrieving repo content\", e)\\n',\n",
       " 'src/__init__.py': '',\n",
       " 'src/github_utils.py': 'from github import Github\\n\\ndef get_all_files(repo, path=\"\"):\\n    files = {}\\n    contents = repo.get_contents(path)\\n    for content in contents:\\n        if content.type == \"dir\":\\n            files[content.path] = get_all_files(repo, content.path)\\n        else:\\n            files[content.path] = content.content # base64\\n    return files'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_files_flatten = gu.flatten(decoded_files)\n",
    "decoded_files_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SourceCodeLoader with the decoded files\n",
    "class CustomCodeLoader(BaseLoader):\n",
    "    # Mapping of file extensions to their corresponding languages\n",
    "    extension_to_language = {\n",
    "        '.py': 'python',\n",
    "        '.ipynb': 'jupyter notebook',\n",
    "        '.r': 'R',\n",
    "        '.js': 'javascript',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def get_language_from_extension(self, file_path):\n",
    "        _, extension = os.path.splitext(file_path)\n",
    "        return self.extension_to_language.get(extension.lower(), 'unknown')\n",
    "\n",
    "    def load(self):\n",
    "        for path, content in self.files.items():\n",
    "            if isinstance(content, str):\n",
    "                language = self.get_language_from_extension(path)\n",
    "                document = {\n",
    "                    'page_content': content,\n",
    "                    'metadata': {\n",
    "                        'path': path,\n",
    "                        'language': language\n",
    "                    }\n",
    "                }\n",
    "                yield document\n",
    "                \n",
    "custom_loader = CustomCodeLoader(decoded_files_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitignore unknown\n",
      "LICENSE unknown\n",
      "README.md unknown\n",
      "config/logs/local.conf unknown\n",
      "requirements.txt unknown\n",
      "sandbox.py python\n",
      "src/__init__.py python\n",
      "src/github_utils.py python\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF CUSTOM LOADER\n",
    "for document in custom_loader.load():\n",
    "    # Each document's 'content' is the file content\n",
    "    # and 'metadata' contains the file path and deduced language\n",
    "    print(document['metadata']['path'], document['metadata']['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': '.env\\n.venv/\\n.vscode/\\n*.log\\n*.pyc',\n",
       "  'metadata': {'path': '.gitignore', 'language': 'unknown'}},\n",
       " {'page_content': 'MIT License\\n\\nCopyright (c) 2023 Hye Won (Nicole) Hwang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n',\n",
       "  'metadata': {'path': 'LICENSE', 'language': 'unknown'}},\n",
       " {'page_content': \"# Auto-Readme\\n\\nAutomatically generate `readme.md` files for your repositories by parsing the code and summarizing it using Openai API.\\n\\n## Features\\n\\n- Parse GitHub repositories without downloading them.\\n- Summarize code using Langchain.\\n- Generate human-readable summaries with OpenAI's GPT model.\\n\\n## Prerequisites\\n\\n- Python 3.8+\\n- GitHub API Token (https://github.com/settings/tokens)\\n- OpenAI API Key (https://beta.openai.com/signup)\\n\\n## Installation\\n\\n1. Clone Repository\\n2. Create a virtual environment `python -m venv .venv` and install packages in `requirements.txt`\\n2. Create .env file at the root of directory\\n\\n```bash\\nGITHUB_API_TOKEN=<add-token-here>\\nOPENAI_API_TOKEN=<add-token-here>\\n```\\n\",\n",
       "  'metadata': {'path': 'README.md', 'language': 'unknown'}},\n",
       " {'page_content': \"[loggers]\\nkeys=root,src\\n\\n[handlers]\\nkeys=consoleHandler,fileHandler\\n\\n[formatters]\\nkeys=sampleFormatter\\n\\n[logger_root]\\nlevel=INFO\\nhandlers=consoleHandler,fileHandler\\npropagate=0\\n\\n[logger_src]\\nlevel=INFO\\nhandlers=consoleHandler,fileHandler\\nqualname=src\\npropagate=0\\n\\n[handler_consoleHandler]\\nclass=StreamHandler\\nlevel=INFO\\nformatter=sampleFormatter\\nargs=(sys.stdout,)\\n\\n[handler_fileHandler]\\nclass=FileHandler\\nlevel=INFO\\nformatter=sampleFormatter\\nargs=('config/logs/logs.log',)\\n\\n[formatter_sampleFormatter]\\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\\ndatefmt=%Y-%m-%d %H:%M:%S\",\n",
       "  'metadata': {'path': 'config/logs/local.conf', 'language': 'unknown'}},\n",
       " {'page_content': 'aiohttp==3.8.6\\naiosignal==1.3.1\\nasync-timeout==4.0.3\\nattrs==23.1.0\\ncertifi==2023.7.22\\ncffi==1.16.0\\ncharset-normalizer==3.3.2\\ncryptography==41.0.5\\nDeprecated==1.2.14\\nfrozenlist==1.4.0\\nidna==3.4\\nmultidict==6.0.4\\nopenai==0.28.1\\npycparser==2.21\\nPyGithub==2.1.1\\nPyJWT==2.8.0\\nPyNaCl==1.5.0\\npython-dateutil==2.8.2\\npython-dotenv==1.0.0\\nrequests==2.31.0\\nsix==1.16.0\\ntqdm==4.66.1\\ntyping_extensions==4.8.0\\nurllib3==2.0.7\\nwrapt==1.15.0\\nyarl==1.9.2\\n',\n",
       "  'metadata': {'path': 'requirements.txt', 'language': 'unknown'}},\n",
       " {'page_content': 'import os\\nimport argparse\\nimport logging\\nimport logging.config\\n\\nfrom dotenv import load_dotenv\\nfrom github import Github\\nimport src.github_utils as gu\\n\\nlogging.config.fileConfig(\"config/logs/local.conf\")\\nlogger = logging.getLogger(__name__)\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser(\\n        description=\"Auto-readme\"\\n    )\\n    parser.add_argument(\\n        \"-r\", \"--repo\", default=\"hyeniii/auto-readme\", help=\"Specify github repository to create a readme on\"\\n    )\\n    args = parser.parse_args()\\n\\n    # Load environment variables from .env file\\n    load_dotenv()\\n\\n    GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\\n    OPEN_API_TOKEN = os.getenv(\"OPENAI_API_TOKEN\")\\n\\n    if GITHUB_API_TOKEN is None or OPEN_API_TOKEN is None:\\n        logger.error(\"API tokens not found in environment variables.\")\\n        exit(1)  # Exit the program if tokens are not set\\n\\n    try:\\n        g = Github(GITHUB_API_TOKEN)\\n        repo = g.get_repo(\"public-apis/public-apis\")\\n        print(repo.get_topics())\\n        logging.info(\"Recetriving data from %s\", repo.full_name)\\n        contents = repo.get_contents(\"\")\\n        all_files = gu.get_all_files(repo)\\n        print(all_files)\\n    except Exception as e:\\n        logging.error(\"Error occured in retrieving repo content\", e)\\n',\n",
       "  'metadata': {'path': 'sandbox.py', 'language': 'python'}},\n",
       " {'page_content': '',\n",
       "  'metadata': {'path': 'src/__init__.py', 'language': 'python'}},\n",
       " {'page_content': 'from github import Github\\n\\ndef get_all_files(repo, path=\"\"):\\n    files = {}\\n    contents = repo.get_contents(path)\\n    for content in contents:\\n        if content.type == \"dir\":\\n            files[content.path] = get_all_files(repo, content.path)\\n        else:\\n            files[content.path] = content.content # base64\\n    return files',\n",
       "  'metadata': {'path': 'src/github_utils.py', 'language': 'python'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = list(custom_loader.load())\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```\\n.\\n├── .gitignore\\n├── LICENSE\\n├── README.md\\n├── config\\n│   └── logs\\n│       └── local.conf\\n├── requirements.txt\\n├── sandbox.py\\n└── src\\n    ├── __init__.py\\n    └── github_utils.py\\n```'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- OUTPUT PARSER ---    \n",
    "class MarkdownTreeStructureOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to format the repository structure as a tree suitable for Markdown.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        # Split the text into lines, each representing a file path\n",
    "        paths = text.split(\"\\n\")\n",
    "\n",
    "        # Organize paths into a tree structure\n",
    "        tree = self.build_tree_structure(paths)\n",
    "\n",
    "        # Format the tree structure for Markdown output\n",
    "        formatted_text = \"```\\n\" + self.format_tree(tree) + \"```\"\n",
    "        return formatted_text\n",
    "\n",
    "    def build_tree_structure(self, paths):\n",
    "        \"\"\"Build a tree structure from a list of paths.\"\"\"\n",
    "        tree = {}\n",
    "        for path in paths:\n",
    "            current_level = tree\n",
    "            for part in path.split('/'):\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {}\n",
    "                current_level = current_level[part]\n",
    "        return tree\n",
    "\n",
    "    def format_tree(self, tree, indent=0):\n",
    "        \"\"\"Recursively format the tree structure for output.\"\"\"\n",
    "        output = \"\"\n",
    "        for key, value in tree.items():\n",
    "            output += \"    \" * indent + f\"{key}\\n\"  # Adjust the indentation if needed\n",
    "            if isinstance(value, dict):\n",
    "                output += self.format_tree(value, indent + 1)\n",
    "        return output\n",
    "\n",
    "# --- PROMPT TEMPLATE: SUMMARIZE REPO ---    \n",
    "template = \"\"\"\n",
    "You are a helpful assistant who helps to build a README file. \n",
    "Return your answer as a tree generator with no further explanations. \n",
    "\"\"\"\n",
    "human_template = 'This are the list of paths of the reposoitory files: {list_of_paths}. Give me the repo structure.'\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chain = chat_prompt | ChatOpenAI(openai_api_key=OPEN_API_TOKEN) | MarkdownTreeStructureOutputParser()\n",
    "chain.invoke({\"list_of_paths\": [document[\"metadata\"][\"path\"] for document in documents]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To get started with the hyeniii/auto-readme repository, you'll need to follow these steps:\\n\\n## Cloning the Repository\\n1. Open your terminal or command prompt.\\n2. Change the current working directory to the location where you want to clone the repository.\\n3. Run the following command to clone the repository:\\n   ```\\n   git clone https://github.com/hyeniii/auto-readme.git\\n   ```\\n4. Once the cloning process is complete, change the current working directory to the cloned repository:\\n   ```\\n   cd auto-readme\\n   ```\\n\\n## Installing Dependencies\\n1. Make sure you have Python 3.8 or higher installed on your system. If not, please install it before proceeding.\\n2. Create a virtual environment by running the following command:\\n   ```\\n   python -m venv .venv\\n   ```\\n3. Activate the virtual environment:\\n   - For Windows:\\n     ```\\n     .venv\\\\Scripts\\\\activate\\n     ```\\n   - For macOS and Linux:\\n     ```\\n     source .venv/bin/activate\\n     ```\\n4. Install the required packages by running the following command:\\n   ```\\n   pip install -r requirements.txt\\n   ```\\n5. Create a `.env` file at the root of the directory and add the following content:\\n   ```\\n   GITHUB_API_TOKEN=<add-github-api-token-here>\\n   OPENAI_API_TOKEN=<add-openai-api-token-here>\\n   ```\\n   Replace `<add-github-api-token-here>` with your GitHub API token, and `<add-openai-api-token-here>` with your OpenAI API token. You can obtain these tokens from the respective links provided in the repository's README file.\\n6. You're now ready to use the repository and its dependencies.\\n\\nPlease note that you should replace `<add-github-api-token-here>` and `<add-openai-api-token-here>` with your actual API tokens.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FormattedOutputConverToText(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to format as a simple string\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        \n",
    "        # Format the file path in bold and the summary as a paragraph\n",
    "        formatted_text = f\"{text}\"\n",
    "        return formatted_text\n",
    "    \n",
    "\n",
    "# --- PROMPT TEMPLATE: SUMMARIZE REPO ---    \n",
    "template = \"\"\"\n",
    "You are a helpful assistant who helps built a README file for a Github repository. \n",
    "Give me instructions on how to to get started. detailing the steps for cloning the repository \n",
    "and the steps for installing dependencies. \"\n",
    "\n",
    "\"\"\"\n",
    "human_template = \"Repo Name: {repo}. List of files: {file_paths}. Content of files: {contents} \"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "#chat_prompt.format_messages(language = d1[\"metatdata\"][\"language\"], code = d1[\"page_content\"])\n",
    "    \n",
    "\n",
    "chain = chat_prompt | ChatOpenAI(openai_api_key=OPEN_API_TOKEN) | FormattedOutputConverToText()\n",
    "chain.invoke({\"repo\": repo_name, \"file_paths\": [doc[\"metadata\"][\"path\"] for doc in documents],\"contents\": [doc[\"page_content\"] for doc in documents]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file summary: example for one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: github_utils.py\n",
    "d1 = documents[7]\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OUTPUT PARSER ---    \n",
    "class FormattedOutputParserSummary(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to format the file path in bold and the summarization as a paragraph.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        # Assuming the text format is \"File Path: {file_path} Summary: {summary}\"\n",
    "        # Adjust the split logic based on the actual format you expect from the LLM\n",
    "        parts = text.split(\"\\nSummary: \")\n",
    "        file_path = parts[0].replace(\"File Path: \", \"\").strip()\n",
    "        summary = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "        # Format the file path in bold and the summary as a paragraph\n",
    "        formatted_text = f\"**{file_path}** \\n\\n{summary}\"\n",
    "        return formatted_text\n",
    "    \n",
    "\n",
    "# --- PROMPT TEMPLATE: SUMMARIZE REPO ---    \n",
    "template = \"\"\"\n",
    "You are a helpful assistant who generates summarizations of code to build a README file. \n",
    "Return your response in the format File Path: the_file_path Summary: the_summary\n",
    "\"\"\"\n",
    "human_template = \"The file {path} has this content: {content}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "#chat_prompt.format_messages(language = d1[\"metatdata\"][\"language\"], code = d1[\"page_content\"])\n",
    "    \n",
    "\n",
    "chain = chat_prompt | ChatOpenAI(openai_api_key=OPEN_API_TOKEN) | FormattedOutputParserSummary()\n",
    "chain.invoke({\"path\": d1[\"metadata\"][\"path\"],\"content\": d1[\"page_content\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file summaries: Example for whole repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'documents' is your list of documents\n",
    "summaries = []\n",
    "\n",
    "for document in documents:\n",
    "    # Extract the path and code content from each document\n",
    "    path = document[\"metadata\"][\"path\"]\n",
    "    content = document[\"page_content\"]\n",
    "\n",
    "    # Invoke the chain for each document\n",
    "    response = chain.invoke({\"path\": path, \"content\": content})\n",
    "\n",
    "    # Append the generated summary to the summaries list\n",
    "    summaries.append(response)\n",
    "\n",
    "summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The repository contains code for an Auto-Readme tool. This tool automatically generates `readme.md` files for repositories by parsing the code and summarizing it using the OpenAI API. It has features such as parsing GitHub repositories without downloading them, summarizing code using Langchain, and generating human-readable summaries with OpenAI's GPT model. The prerequisites for using this tool include Python 3.8+, a GitHub API Token, and an OpenAI API Key. The installation process involves cloning the repository, creating a virtual environment, installing the required packages, and creating a `.env` file with the API tokens. The code files in the repository include a `.gitignore` file, a `LICENSE` file, a `README.md` file with detailed instructions, a logging configuration file, a `requirements.txt` file with the required dependencies, a `sandbox.py` file containing the main functionality of the tool, an `__init__.py` file, and a `github_utils.py` file with utility functions for working with GitHub repositories.\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- PROMPT TEMPLATE: SUMMARIZE REPO ---    \n",
    "template = \"\"\"\n",
    "You are a helpful assistant who generates summarizations of code to build a README file. \n",
    "Give me a one paragraph with a brief overview of what is the repo for. \n",
    "\"\"\"\n",
    "human_template = \"Repo content: {documents}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "#chat_prompt.format_messages(language = d1[\"metatdata\"][\"language\"], code = d1[\"page_content\"])\n",
    "    \n",
    "\n",
    "chain = chat_prompt | ChatOpenAI(openai_api_key=OPEN_API_TOKEN) | FormattedOutputConverToText()\n",
    "#| FormattedOutputParserSummary()\n",
    "chain.invoke({\"documents\": documents})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
